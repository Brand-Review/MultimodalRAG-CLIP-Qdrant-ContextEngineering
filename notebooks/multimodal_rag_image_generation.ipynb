{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal RAG with Image Generation\n",
        "\n",
        "This notebook demonstrates a complete multimodal RAG (Retrieval-Augmented Generation) system that combines:\n",
        "\n",
        "- **Image and Text Embeddings**: Using CLIP for images and multilingual E5 for text\n",
        "- **Vector Database**: Qdrant for efficient similarity search\n",
        "- **Content Generation**: Qwen2.5-VL for generating descriptions from retrieved examples\n",
        "- **Advanced Retrieval**: MMR (Maximal Marginal Relevance) and deduplication\n",
        "\n",
        "## Features\n",
        "- Multi-modal search (text-to-image, image-to-image)\n",
        "- Cross-modal reranking using CLIP\n",
        "- MMR diversification to avoid redundant results\n",
        "- Content generation inspired by retrieved examples\n",
        "- Support for multilingual text (including Bangla)\n",
        "\n",
        "## Table of Contents\n",
        "1. [Setup and Installation](#setup)\n",
        "2. [Configuration](#config)\n",
        "3. [Data Loading and Utilities](#data)\n",
        "4. [Embedding Models](#embeddings)\n",
        "5. [Vector Database (Qdrant)](#qdrant)\n",
        "6. [Data Ingestion](#ingestion)\n",
        "7. [Search and Retrieval](#search)\n",
        "8. [Content Generation](#generation)\n",
        "9. [Complete Workflow Example](#workflow)\n",
        "10. [Advanced Features](#advanced)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation {#setup}\n",
        "\n",
        "First, let's install the required dependencies and import the necessary libraries.\n",
        "\n",
        "**Required packages:**\n",
        "- `sentence-transformers`: For embedding models (CLIP, E5)\n",
        "- `qdrant-client`: Vector database client\n",
        "- `transformers`: For the Qwen2.5-VL generation model\n",
        "- `torch`: PyTorch for deep learning\n",
        "- `pillow`: Image processing\n",
        "- `tqdm`: Progress bars\n",
        "- `numpy`: Numerical operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: sentence-transformers in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (5.1.1)\n",
            "Requirement already satisfied: qdrant-client in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (1.15.1)\n",
            "Requirement already satisfied: pandas in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (2.3.3)\n",
            "Requirement already satisfied: torch in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (2.8.0)\n",
            "Requirement already satisfied: torchvision in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (0.23.0)\n",
            "Requirement already satisfied: torchaudio in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (2.8.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from sentence-transformers) (1.7.2)\n",
            "Requirement already satisfied: scipy in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from qdrant-client) (1.75.1)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from qdrant-client) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from qdrant-client) (6.32.1)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from qdrant-client) (2.11.9)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: anyio in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.11.0)\n",
            "Requirement already satisfied: certifi in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run this cell if packages are not installed)\n",
        "import sys\n",
        "!{sys.executable} -m pip install -U sentence-transformers qdrant-client pandas torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# Core imports\n",
        "import os\n",
        "import uuid\n",
        "import json\n",
        "import math\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Embedding models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Vector database\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
        "\n",
        "# Generation model\n",
        "from transformers import AutoProcessor\n",
        "from transformers.models.qwen2_5_vl import Qwen2_5_VLForConditionalGeneration\n",
        "\n",
        "print(\"All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration {#config}\n",
        "\n",
        "Define the configuration parameters for our multimodal RAG system.\n",
        "\n",
        "**Key Components:**\n",
        "- **CLIP ViT-L/14**: For image embeddings and cross-modal text-image understanding\n",
        "- **Multilingual E5**: For text embeddings with support for multiple languages including Bangla\n",
        "- **Qdrant**: In-memory vector database for this demo (can be configured for server mode)\n",
        "- **Qwen2.5-VL-7B**: Vision-language model for content generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Embedding Models\n",
        "IMG_EMB = \"sentence-transformers/clip-ViT-L-14\"  # CLIP for cross-modal embeddings\n",
        "TXT_EMB = \"intfloat/multilingual-e5-base\"        # Multilingual text embeddings (Bangla-friendly)\n",
        "\n",
        "# Vector Database\n",
        "COLLECTION = \"mm_posts\"  # Collection name in Qdrant\n",
        "USE_SERVER = False       # Use in-memory Qdrant for demo\n",
        "QHOST, QPORT = \"localhost\", 6333  # Qdrant server settings\n",
        "\n",
        "# Retrieval Parameters\n",
        "TOPK_INIT = 24          # Initial number of candidates\n",
        "TOPK_FINAL = 6          # Final number after reranking\n",
        "MMR_LAMBDA = 0.7        # MMR diversification factor (0=relevance, 1=diversity)\n",
        "DEDUP_THR = 0.96        # Deduplication threshold (cosine similarity)\n",
        "\n",
        "# Generation Model\n",
        "GEN_MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading and Utilities {#data}\n",
        "\n",
        "This section contains utility functions for data loading, normalization, and basic operations.\n",
        "\n",
        "**Key Functions:**\n",
        "- `load_json_any()`: Flexible JSON/JSONL loading\n",
        "- `norm()`: Vector normalization for cosine similarity\n",
        "- Error handling for file operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utility functions defined successfully!\n",
            "Loaded 120 posts\n",
            "            image              date                       caption  \\\n",
            "0  2025-09-23.jpg        2025-09-23             Us on a rainy day   \n",
            "1  2025-09-22.jpg  2025-09-22 08:17  You got the order right Sir!   \n",
            "2                  2025-09-20 21:12  Ekhon Texas eo DeliveryHobe.   \n",
            "3                  2025-09-20 08:04       Esob kore ki moja PAAN?   \n",
            "4                  2025-09-19 06:19                   Bucchen to?   \n",
            "\n",
            "                                         description  \\\n",
            "0  Candid photo of an older man crouching beside ...   \n",
            "1  Black‑and‑white portrait of a man (Rahat) with...   \n",
            "2  X‑ray style graphic showing a human pelvis wit...   \n",
            "3  Flat‑lay photograph of betel leaves arranged o...   \n",
            "4  Bright red graphic featuring a boiled egg and ...   \n",
            "\n",
            "                                          engagement  \n",
            "0     {'reactions': 167, 'comments': 3, 'shares': 6}  \n",
            "1  {'reactions': 3400, 'comments': 21, 'shares': ...  \n",
            "2     {'reactions': 182, 'comments': 5, 'shares': 4}  \n",
            "3   {'reactions': 455, 'comments': 19, 'shares': 12}  \n",
            "4      {'reactions': 79, 'comments': 5, 'shares': 3}  \n"
          ]
        }
      ],
      "source": [
        "def load_json_any(path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load JSON data from file, supporting both JSON array and JSONL formats.\n",
        "    \n",
        "    Args:\n",
        "        path: Path to the JSON/JSONL file\n",
        "        \n",
        "    Returns:\n",
        "        List of dictionaries containing the loaded data\n",
        "        \n",
        "    Example:\n",
        "        # JSON format: [{\"image\": \"img1.jpg\", \"caption\": \"...\"}, ...]\n",
        "        # JSONL format: {\"image\": \"img1.jpg\", \"caption\": \"...\"}\\n{\"image\": \"img2.jpg\", ...}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            txt = f.read().strip()\n",
        "        \n",
        "        if txt.startswith(\"[\"):\n",
        "            # Standard JSON array format\n",
        "            return json.loads(txt)\n",
        "        else:\n",
        "            # JSONL format (one JSON object per line)\n",
        "            return [json.loads(line) for line in txt.splitlines() if line.strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def norm(v: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Normalize vectors for cosine similarity computation.\n",
        "    \n",
        "    Args:\n",
        "        v: Input vectors of shape (n_samples, n_features)\n",
        "        \n",
        "    Returns:\n",
        "        Normalized vectors (L2 norm = 1)\n",
        "        \n",
        "    Note:\n",
        "        Handles zero vectors by setting them to unit vectors\n",
        "    \"\"\"\n",
        "    n = np.linalg.norm(v, axis=1, keepdims=True)\n",
        "    n[n == 0] = 1  \n",
        "    return v / n\n",
        "\n",
        "# Test the utility functions\n",
        "print(\"Utility functions defined successfully!\")\n",
        "\n",
        "\n",
        "posts = load_json_any(\"/Users/sayems_mac/BrandReview/brandReviewModels/RAG_pipeline/posts_data.json\")\n",
        "print(f\"Loaded {len(posts)} posts\")\n",
        "\n",
        "# print some posts as a table\n",
        "import pandas as pd\n",
        "\n",
        "# Convert posts to DataFrame\n",
        "df = pd.DataFrame(posts)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Embedding Models {#embeddings}\n",
        "\n",
        "This section implements the embedding models for both images and text.\n",
        "\n",
        "**Models Used:**\n",
        "- **CLIP ViT-L/14**: For image embeddings and cross-modal text-image understanding\n",
        "- **Multilingual E5**: For text embeddings with excellent multilingual support\n",
        "\n",
        "**Key Features:**\n",
        "- Lazy loading (models loaded only when needed)\n",
        "- Automatic normalization for cosine similarity\n",
        "- Support for both query and passage text encoding (E5 requirement)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Global variables for model caching (lazy loading)\n",
        "IMG = None\n",
        "TXT = None\n",
        "\n",
        "def get_img_encoder():\n",
        "    \"\"\"\n",
        "    Get the image encoder (CLIP ViT-L/14).\n",
        "    Uses global caching to avoid reloading the model.\n",
        "    \n",
        "    Returns:\n",
        "        SentenceTransformer model for image embeddings\n",
        "    \"\"\"\n",
        "    global IMG\n",
        "    if IMG is None:\n",
        "        print(\"Loading CLIP ViT-L/14 model...\")\n",
        "        IMG = SentenceTransformer(IMG_EMB)\n",
        "        print(\"Image encoder loaded successfully!\")\n",
        "    return IMG\n",
        "\n",
        "def get_txt_encoder():\n",
        "    \"\"\"\n",
        "    Get the text encoder (Multilingual E5).\n",
        "    Uses global caching to avoid reloading the model.\n",
        "    \n",
        "    Returns:\n",
        "        SentenceTransformer model for text embeddings\n",
        "    \"\"\"\n",
        "    global TXT\n",
        "    if TXT is None:\n",
        "        print(\"Loading Multilingual E5 model...\")\n",
        "        TXT = SentenceTransformer(TXT_EMB)\n",
        "        print(\"Text encoder loaded successfully!\")\n",
        "    return TXT\n",
        "\n",
        "def emb_image(paths: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of images.\n",
        "    \n",
        "    Args:\n",
        "        paths: List of image file paths\n",
        "        \n",
        "    Returns:\n",
        "        Normalized image embeddings of shape (n_images, embedding_dim)\n",
        "        \n",
        "    Note:\n",
        "        Images are automatically converted to RGB format\n",
        "    \"\"\"\n",
        "    enc = get_img_encoder()\n",
        "    imgs = [Image.open(p).convert(\"RGB\") for p in paths]\n",
        "    return enc.encode(imgs, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "\n",
        "def emb_text(texts: List[str], mode: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate embeddings for text with proper E5 prefixing.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of text strings to embed\n",
        "        mode: Either \"query\" or \"passage\" (affects E5 prefixing)\n",
        "        \n",
        "    Returns:\n",
        "        Normalized text embeddings of shape (n_texts, embedding_dim)\n",
        "        \n",
        "    Note:\n",
        "        E5 model requires specific prefixes: \"query: \" for queries, \"passage: \" for passages\n",
        "    \"\"\"\n",
        "    prefix = \"query: \" if mode == \"query\" else \"passage: \"\n",
        "    enc = get_txt_encoder()\n",
        "    prefixed_texts = [prefix + t for t in texts]\n",
        "    return enc.encode(prefixed_texts, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "\n",
        "print(\"Embedding functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Vector Database (Qdrant) {#qdrant}\n",
        "\n",
        "This section implements the vector database operations using Qdrant.\n",
        "\n",
        "**Qdrant Features:**\n",
        "- **Multi-vector support**: Separate vectors for images and text\n",
        "- **Cosine similarity**: Optimized for normalized embeddings\n",
        "- **In-memory mode**: For demo purposes (can be configured for server mode)\n",
        "- **Collection management**: Automatic recreation and cleanup\n",
        "\n",
        "**Collection Structure:**\n",
        "- **image**: CLIP embeddings (768 dimensions)\n",
        "- **text**: E5 embeddings (768 dimensions)\n",
        "- **payload**: Metadata (image path, caption, description, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qdrant functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Global Qdrant client (lazy loading)\n",
        "_Q = None\n",
        "\n",
        "def qdrant() -> QdrantClient:\n",
        "    \"\"\"\n",
        "    Get Qdrant client instance.\n",
        "    Uses global caching and supports both in-memory and server modes.\n",
        "    \n",
        "    Returns:\n",
        "        QdrantClient instance\n",
        "    \"\"\"\n",
        "    global _Q\n",
        "    if _Q is None:\n",
        "        if USE_SERVER:\n",
        "            print(f\"Connecting to Qdrant server at {QHOST}:{QPORT}\")\n",
        "            _Q = QdrantClient(host=QHOST, port=QPORT)\n",
        "        else:\n",
        "            print(\"Using in-memory Qdrant database\")\n",
        "            _Q = QdrantClient(\":memory:\")\n",
        "    return _Q\n",
        "\n",
        "def recreate_collection(dim_img: int, dim_txt: int):\n",
        "    \"\"\"\n",
        "    Create or recreate the vector collection with specified dimensions.\n",
        "    \n",
        "    Args:\n",
        "        dim_img: Dimension of image embeddings\n",
        "        dim_txt: Dimension of text embeddings\n",
        "        \n",
        "    Note:\n",
        "        This will delete any existing collection with the same name\n",
        "    \"\"\"\n",
        "    qc = qdrant()\n",
        "    \n",
        "    # Check if collection exists and delete it\n",
        "    try:\n",
        "        existing_collections = [c.name for c in qc.get_collections().collections]\n",
        "        if COLLECTION in existing_collections:\n",
        "            print(f\"Deleting existing collection '{COLLECTION}'\")\n",
        "            qc.delete_collection(COLLECTION)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not check existing collections: {e}\")\n",
        "    \n",
        "    # Create new collection\n",
        "    print(f\"Creating collection '{COLLECTION}' with image dim={dim_img}, text dim={dim_txt}\")\n",
        "    qc.recreate_collection(\n",
        "        collection_name=COLLECTION,\n",
        "        vectors_config={\n",
        "            \"image\": VectorParams(size=dim_img, distance=Distance.COSINE),\n",
        "            \"text\": VectorParams(size=dim_txt, distance=Distance.COSINE),\n",
        "        }\n",
        "    )\n",
        "    print(f\"Collection '{COLLECTION}' created successfully!\")\n",
        "\n",
        "def upsert(points: List[PointStruct]):\n",
        "    \"\"\"\n",
        "    Insert or update points in the collection.\n",
        "    \n",
        "    Args:\n",
        "        points: List of PointStruct objects containing vectors and payloads\n",
        "    \"\"\"\n",
        "    qdrant().upsert(collection_name=COLLECTION, points=points)\n",
        "    print(f\"Upserted {len(points)} points to collection '{COLLECTION}'\")\n",
        "\n",
        "print(\"Qdrant functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Ingestion {#ingestion}\n",
        "\n",
        "This section implements the data ingestion pipeline that processes posts and creates embeddings.\n",
        "\n",
        "**Ingestion Process:**\n",
        "1. **Load Data**: Parse JSON/JSONL files containing posts\n",
        "2. **Prepare Images**: Extract image paths and load images\n",
        "3. **Generate Embeddings**: Create both image and text embeddings\n",
        "4. **Create Collection**: Set up Qdrant collection with proper dimensions\n",
        "5. **Store Data**: Insert embeddings with metadata into vector database\n",
        "\n",
        "**Data Format Expected:**\n",
        "```json\n",
        "{\n",
        "  \"image\": \"filename.jpg\",\n",
        "  \"caption\": \"Image caption\",\n",
        "  \"description\": \"Detailed description\",\n",
        "  \"additional_field\": \"any other metadata\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingestion function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def ingest(posts_json: str, images_dir):\n",
        "    \"\"\"\n",
        "    Complete ingestion pipeline for multimodal posts.\n",
        "    \n",
        "    Args:\n",
        "        posts_json: Path to JSON/JSONL file containing posts\n",
        "        images_dir: Directory containing the image files\n",
        "        \n",
        "    Process:\n",
        "        1. Load and parse posts data\n",
        "        2. Extract image paths and prepare text content\n",
        "        3. Generate image embeddings using CLIP\n",
        "        4. Generate text embeddings using E5\n",
        "        5. Create Qdrant collection with proper dimensions\n",
        "        6. Store all data as points in the vector database\n",
        "    \"\"\"\n",
        "    print(\"=== Starting Data Ingestion ===\")\n",
        "\n",
        "    print(\"luffy\",images_dir)\n",
        "    \n",
        "    # Step 1: Load posts data\n",
        "    print(f\"Loading posts from {posts_json}...\")\n",
        "    posts = load_json_any(posts_json)\n",
        "\n",
        "    if not posts:\n",
        "        print(\"No posts loaded. Exiting.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Loaded {len(posts)} posts\")\n",
        "    \n",
        "    # Step 2: Prepare data\n",
        "    print(\"Preparing image paths and text content...\")\n",
        "    paths = images_dir\n",
        "    caps = [p.get(\"caption\", \"\") for p in posts]\n",
        "    descs = [p.get(\"description\", \"\") for p in posts]\n",
        "    \n",
        "    # Combine caption and description for text embedding\n",
        "    texts = [f\"caption: {c}\\n\\ndescription: {d}\" for c, d in zip(caps, descs)]\n",
        "    \n",
        "    # Step 3: Generate embeddings\n",
        "    print(f\"Generating embeddings for {len(paths)} images...\")\n",
        "    v_img = emb_image(paths)\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(texts)} text descriptions...\")\n",
        "    v_txt = emb_text(texts, mode=\"passage\")\n",
        "    \n",
        "    print(f\"Image embeddings shape: {v_img.shape}\")\n",
        "    print(f\"Text embeddings shape: {v_txt.shape}\")\n",
        "    \n",
        "    # Step 4: Create collection\n",
        "    recreate_collection(v_img.shape[1], v_txt.shape[1])\n",
        "    \n",
        "    # Step 5: Create points and store\n",
        "    print(\"Creating vector database points...\")\n",
        "    points = []\n",
        "    \n",
        "    for vi, vt, p, c, d in zip(v_img, v_txt, posts, caps, descs):\n",
        "        # Create payload with all original data\n",
        "        payload = {\n",
        "            \"image\": p[\"image\"],\n",
        "            \"caption\": c,\n",
        "            \"description\": d,\n",
        "            # Include any additional fields from the original post\n",
        "            **{k: v for k, v in p.items() if k not in [\"image\", \"caption\", \"description\"]}\n",
        "        }\n",
        "        \n",
        "        # Create point with both image and text vectors\n",
        "        point = PointStruct(\n",
        "            id=str(uuid.uuid4()),\n",
        "            vector={\"image\": vi.tolist(), \"text\": vt.tolist()},\n",
        "            payload=payload\n",
        "        )\n",
        "        points.append(point)\n",
        "    \n",
        "    # Store in vector database\n",
        "    upsert(points)\n",
        "    print(f\"=== Ingestion Complete: {len(points)} items stored ===\")\n",
        "\n",
        "print(\"Ingestion function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Search and Retrieval {#search}\n",
        "\n",
        "This section implements advanced search and retrieval capabilities with reranking and diversification.\n",
        "\n",
        "**Search Types:**\n",
        "- **Text-to-Image**: Find images similar to a text query\n",
        "- **Image-to-Image**: Find images similar to a query image\n",
        "\n",
        "**Advanced Features:**\n",
        "- **Cross-modal Reranking**: Use CLIP for text-image similarity\n",
        "- **MMR Diversification**: Avoid redundant results using Maximal Marginal Relevance\n",
        "- **Deduplication**: Remove highly similar results\n",
        "- **Multi-stage Retrieval**: Initial ANN search + reranking + diversification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search and retrieval functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def search_by_text(query: str, topk: int) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Search for images using a text query.\n",
        "    \n",
        "    Args:\n",
        "        query: Text query string\n",
        "        topk: Number of results to return\n",
        "        \n",
        "    Returns:\n",
        "        List of search results with scores and payloads\n",
        "    \"\"\"\n",
        "    qv = emb_text([query], mode=\"query\")[0]\n",
        "    res = qdrant().search(\n",
        "        collection_name=COLLECTION,\n",
        "        query_vector=(\"text\", qv.tolist()),\n",
        "        limit=topk,\n",
        "        with_payload=True\n",
        "    )\n",
        "    return [{\"score\": r.score, \"payload\": r.payload} for r in res]\n",
        "\n",
        "def search_by_image(img_path: str, topk: int) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Search for similar images using an image query.\n",
        "    \n",
        "    Args:\n",
        "        img_path: Path to the query image\n",
        "        topk: Number of results to return\n",
        "        \n",
        "    Returns:\n",
        "        List of search results with scores and payloads\n",
        "    \"\"\"\n",
        "    qv = emb_image([img_path])[0]\n",
        "    res = qdrant().search(\n",
        "        collection_name=COLLECTION,\n",
        "        query_vector=(\"image\", qv.tolist()),\n",
        "        limit=topk,\n",
        "        with_payload=True\n",
        "    )\n",
        "    return [{\"score\": r.score, \"payload\": r.payload} for r in res]\n",
        "\n",
        "def cosine(a, b):\n",
        "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "def mmr_diversify(vecs: np.ndarray, base_scores: np.ndarray, k: int, lam: float) -> List[int]:\n",
        "    \"\"\"\n",
        "    Apply Maximal Marginal Relevance (MMR) for result diversification.\n",
        "    \n",
        "    Args:\n",
        "        vecs: Normalized vectors (n_samples, n_features)\n",
        "        base_scores: Relevance scores (n_samples,)\n",
        "        k: Number of diverse results to select\n",
        "        lam: Diversification factor (0=relevance only, 1=diversity only)\n",
        "        \n",
        "    Returns:\n",
        "        List of indices for diverse results\n",
        "        \n",
        "    Algorithm:\n",
        "        MMR = λ * Relevance - (1-λ) * max_similarity_to_selected\n",
        "    \"\"\"\n",
        "    chosen = []\n",
        "    candidates = list(range(len(vecs)))\n",
        "    \n",
        "    while candidates and len(chosen) < k:\n",
        "        if not chosen:\n",
        "            # Select the most relevant item first\n",
        "            i = int(np.argmax(base_scores[candidates]))\n",
        "            chosen.append(candidates.pop(candidates.index(i)))\n",
        "            continue\n",
        "        \n",
        "        # Compute MMR scores for remaining candidates\n",
        "        selected_vecs = np.stack([vecs[i] for i in chosen], 0)  # (m, d)\n",
        "        similarities = vecs[candidates] @ selected_vecs.T  # (c, m)\n",
        "        \n",
        "        # MMR formula\n",
        "        mmr_scores = lam * base_scores[candidates] - (1 - lam) * np.max(similarities, axis=1)\n",
        "        \n",
        "        # Select the item with highest MMR score\n",
        "        i = candidates[int(np.argmax(mmr_scores))]\n",
        "        candidates.remove(i)\n",
        "        chosen.append(i)\n",
        "    \n",
        "    return chosen\n",
        "\n",
        "def dedup_keep(vecs: np.ndarray, threshold: float) -> List[int]:\n",
        "    \"\"\"\n",
        "    Remove duplicate vectors based on cosine similarity threshold.\n",
        "    \n",
        "    Args:\n",
        "        vecs: Normalized vectors (n_samples, n_features)\n",
        "        threshold: Similarity threshold for deduplication\n",
        "        \n",
        "    Returns:\n",
        "        List of indices to keep (removing duplicates)\n",
        "    \"\"\"\n",
        "    keep = []\n",
        "    for i, v in enumerate(vecs):\n",
        "        # Check if this vector is too similar to any already kept vector\n",
        "        is_duplicate = any(float(v @ vecs[j]) >= threshold for j in keep)\n",
        "        if not is_duplicate:\n",
        "            keep.append(i)\n",
        "    return keep\n",
        "\n",
        "print(\"Search and retrieval functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Advanced post-processing function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def post_process_for_text_query(query: str, hits: List[Dict[str, Any]], images_dir) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Advanced post-processing for text queries with cross-modal reranking, MMR, and deduplication.\n",
        "    \n",
        "    Args:\n",
        "        query: Original text query\n",
        "        hits: Initial search results from text vector search\n",
        "        images_dir: Directory containing images (required for image path resolution)\n",
        "        \n",
        "    Returns:\n",
        "        Reranked and diversified results\n",
        "        \n",
        "    Process:\n",
        "        1. Extract image paths from hits\n",
        "        2. Generate CLIP embeddings for images\n",
        "        3. Rerank using CLIP text-image similarity\n",
        "        4. Apply MMR diversification\n",
        "        5. Remove duplicates\n",
        "    \"\"\"\n",
        "    if not hits:\n",
        "        return []\n",
        "    \n",
        "    if images_dir is None:\n",
        "        print(\"Warning: images_dir not provided, using original hits without reranking\")\n",
        "        return hits\n",
        "    \n",
        "    print(f\"Post-processing {len(hits)} hits with cross-modal reranking...\")\n",
        "    \n",
        "    # Step 1: Extract image paths and generate CLIP embeddings\n",
        "    img_paths = [h[\"payload\"][\"image\"] for h in hits]\n",
        "    full_img_paths = images_dir\n",
        "    img_vecs = emb_image(full_img_paths)\n",
        "    \n",
        "    # Step 2: Cross-modal reranking using CLIP\n",
        "    # Use CLIP's text encoder to get query embedding\n",
        "    clip_txt = SentenceTransformer(IMG_EMB)  # CLIP text tower\n",
        "    q_clip = clip_txt.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")[0]\n",
        "    \n",
        "    # Compute text-image similarities\n",
        "    relevance_scores = img_vecs @ q_clip  # cosine similarity (both normalized)\n",
        "    \n",
        "    # Step 3: Apply MMR diversification\n",
        "    final_k = min(TOPK_FINAL, len(hits))\n",
        "    diverse_indices = mmr_diversify(img_vecs, relevance_scores, final_k, MMR_LAMBDA)\n",
        "    \n",
        "    # Step 4: Deduplication\n",
        "    selected_vecs = img_vecs[diverse_indices]\n",
        "    keep_indices = dedup_keep(selected_vecs, DEDUP_THR)\n",
        "    \n",
        "    # Map back to original hit indices\n",
        "    final_indices = [diverse_indices[i] for i in keep_indices]\n",
        "    \n",
        "    print(f\"Selected {len(final_indices)} diverse, non-duplicate results\")\n",
        "    return [hits[i] for i in final_indices]\n",
        "\n",
        "print(\"Advanced post-processing function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Content Generation {#generation}\n",
        "\n",
        "This section implements content generation using the Qwen2.5-VL vision-language model.\n",
        "\n",
        "**Generation Features:**\n",
        "- **Retrieval-Augmented**: Uses retrieved examples as context\n",
        "- **Vision-Language Model**: Qwen2.5-VL for understanding both images and text\n",
        "- **Style Consistency**: Generates content that matches the style of examples\n",
        "- **Brand-Safe Content**: Configured for appropriate social media content\n",
        "\n",
        "**Generation Process:**\n",
        "1. **Context Preparation**: Format retrieved examples as context\n",
        "2. **Model Loading**: Load Qwen2.5-VL with appropriate settings\n",
        "3. **Prompt Construction**: Create structured prompt with examples and new image\n",
        "4. **Content Generation**: Generate description following the established style\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content generation function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import torch for the generation function\n",
        "import torch\n",
        "\n",
        "# Generation style prompt\n",
        "STYLE_PROMPT = (\n",
        "    \"You are a social content writer. Write ONE vivid sentence (15–30 words), \"\n",
        "    \"brand-safe, lightly witty, grounded in the new image/caption and inspired by prior examples. \"\n",
        "    \"No hashtags/emojis; do not invent facts beyond what's visible or stated.\"\n",
        ")\n",
        "\n",
        "# def gen_with_qwen_vl(new_image_path: str, new_caption: str, examples: List[Dict[str, Any]]) -> str:\n",
        "#     \"\"\"\n",
        "#     Generate content using Qwen2.5-VL with retrieved examples as context.\n",
        "    \n",
        "#     Args:\n",
        "#         new_image_path: Path to the new image for content generation\n",
        "#         new_caption: Caption for the new image\n",
        "#         examples: Retrieved examples to use as style reference\n",
        "        \n",
        "#     Returns:\n",
        "#         Generated description text\n",
        "        \n",
        "#     Process:\n",
        "#         1. Load Qwen2.5-VL model and processor\n",
        "#         2. Format examples as context\n",
        "#         3. Create multimodal prompt with image and text\n",
        "#         4. Generate content following the style\n",
        "#     \"\"\"\n",
        "#     print(\"Loading Qwen2.5-VL model...\")\n",
        "    \n",
        "#     # Load model and processor\n",
        "#     processor = AutoProcessor.from_pretrained(GEN_MODEL_ID, trust_remote_code=True)\n",
        "#     model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "#         GEN_MODEL_ID, \n",
        "#         trust_remote_code=True, \n",
        "#         torch_dtype=\"auto\", \n",
        "#         device_map=\"auto\"\n",
        "#     )\n",
        "    \n",
        "#     # Format examples as context\n",
        "#     ex_lines = []\n",
        "#     for i, e in enumerate(examples, 1):\n",
        "#         caption = e['payload'].get('caption', '')\n",
        "#         description = e['payload'].get('description', '')[:200]  # Truncate for brevity\n",
        "#         ex_lines.append(f\"[{i}] CAPTION: {caption}\")\n",
        "#         ex_lines.append(f\"    DESCRIPTION: {description}\")\n",
        "    \n",
        "#     # Create multimodal messages\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": STYLE_PROMPT},\n",
        "#         {\"role\": \"user\", \"content\": [\n",
        "#             {\"type\": \"text\", \"text\": \"EXAMPLES:\\n\" + \"\\n\".join(ex_lines) + \n",
        "#                      f\"\\n\\nNEW CAPTION: {new_caption}\\nWrite the description for this image:\"},\n",
        "#             {\"type\": \"image\", \"image\": Image.open(new_image_path).convert(\"RGB\")}\n",
        "#         ]}\n",
        "#     ]\n",
        "    \n",
        "#     # Generate content\n",
        "#     print(\"Generating content...\")\n",
        "#     inputs = processor.apply_chat_template(\n",
        "#         messages, \n",
        "#         add_generation_prompt=True, \n",
        "#         return_tensors=\"pt\"\n",
        "#     )\n",
        "\n",
        "#     # Check if inputs is a string (tokenized) or dict (raw)\n",
        "#     if isinstance(inputs, str):\n",
        "#             # If it's a string, we need to tokenize it properly\n",
        "#             inputs = processor.tokenizer(\n",
        "#                 inputs, \n",
        "#                 return_tensors=\"pt\", \n",
        "#                 padding=True, \n",
        "#                 truncation=True\n",
        "#             )\n",
        "\n",
        "#     # Move to device\n",
        "#     inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
        "    \n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             **inputs, \n",
        "#             max_new_tokens=96, \n",
        "#             temperature=0.7, \n",
        "#             top_p=0.9, \n",
        "#             do_sample=True\n",
        "#         )\n",
        "    \n",
        "#     # Decode the generated text\n",
        "#     generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
        "    \n",
        "#     # Extract only the generated part (remove the prompt)\n",
        "#     if \"Write the description for this image:\" in generated_text:\n",
        "#         generated_text = generated_text.split(\"Write the description for this image:\")[-1].strip()\n",
        "    \n",
        "#     return generated_text\n",
        "\n",
        "# def gen_with_qwen_vl(new_image_path:str, new_caption:str, examples:List[Dict[str,Any]])->str:\n",
        "#     model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "#     from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "#     processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "#     model = AutoModelForCausalLM.from_pretrained(\n",
        "#         model_id,\n",
        "#         trust_remote_code=True,\n",
        "#         torch_dtype=\"auto\",\n",
        "#         device_map=\"auto\",   # GPU/MPS if available, else CPU\n",
        "#     )\n",
        "\n",
        "#     ex_lines = []\n",
        "#     for i,e in enumerate(examples,1):\n",
        "#         ex_lines.append(f\"[{i}] CAPTION: {e['payload'].get('caption','')}\")\n",
        "#         ex_lines.append(f\"    DESCRIPTION: {e['payload'].get('description','')[:200]}\")\n",
        "\n",
        "#     messages = [\n",
        "#         {\"role\":\"system\",\"content\":STYLE_PROMPT},\n",
        "#         {\"role\":\"user\",\"content\":[\n",
        "#             {\"type\":\"text\",\"text\":\"EXAMPLES:\\n\"+ \"\\n\".join(ex_lines) + f\"\\n\\nNEW CAPTION: {new_caption}\\nWrite the description for this image:\"},\n",
        "#             {\"type\":\"image\",\"image\":Image.open(new_image_path).convert(\"RGB\")}\n",
        "#         ]}\n",
        "#     ]\n",
        "\n",
        "#     print(messages)\n",
        "    \n",
        "#     inputs = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "#     out = model.generate(**inputs, max_new_tokens=96, temperature=0.7, top_p=0.9, do_sample=True)\n",
        "#     text = processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
        "#     return text\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "\n",
        "def gen_with_qwen_vl(new_image_path: str, new_caption: str, examples: List[Dict[str, Any]]) -> str:\n",
        "    model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"  # or 7B\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "    model = AutoModelForVision2Seq.from_pretrained(\n",
        "        model_id,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    ex_lines = []\n",
        "    for i, e in enumerate(examples, 1):\n",
        "        cap = e.get(\"payload\", {}).get(\"caption\", \"\")\n",
        "        desc = e.get(\"payload\", {}).get(\"description\", \"\")[:200]\n",
        "        ex_lines.append(f\"[{i}] CAPTION: {cap}\")\n",
        "        ex_lines.append(f\"    DESCRIPTION: {desc}\")\n",
        "    example_text = \"EXAMPLES:\\n\" + \"\\n\".join(ex_lines)\n",
        "\n",
        "    img = Image.open(new_image_path).convert(\"RGB\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": STYLE_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"text\", \"text\": f\"{example_text}\\n\\nNEW CAPTION: {new_caption}\\nWrite the description for this image:\"},\n",
        "            {\"type\": \"image\", \"image\": img},\n",
        "        ]},\n",
        "    ]\n",
        "\n",
        "    # 1) Build prompt text only (do NOT request tensors here)\n",
        "    prompt_text = processor.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,          # ensure we get a string\n",
        "    )\n",
        "\n",
        "    # 2) Convert to tensors with text+image together\n",
        "    inputs = processor(\n",
        "        text=[prompt_text],\n",
        "        images=[img],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    )\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # 3) Generate\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=96,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        eos_token_id=processor.tokenizer.eos_token_id,\n",
        "        pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    return processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "\n",
        "\n",
        "print(\"Content generation function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Complete Workflow Example {#workflow}\n",
        "\n",
        "This section demonstrates the complete multimodal RAG workflow from data ingestion to content generation.\n",
        "\n",
        "**Workflow Steps:**\n",
        "1. **Data Ingestion**: Load and embed posts data\n",
        "2. **Text Search**: Find similar content using text query\n",
        "3. **Content Generation**: Generate new content using retrieved examples\n",
        "4. **Image Search**: Find similar images using image query\n",
        "5. **Results Display**: Show search results and generated content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complete workflow function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def run_complete_workflow(posts_json: str, images_dir: str, \n",
        "                         query_text: str = None, query_image: str = None,\n",
        "                         generate_image: str = None, generate_caption: str = \"\"):\n",
        "    \"\"\"\n",
        "    Run the complete multimodal RAG workflow.\n",
        "    \n",
        "    Args:\n",
        "        posts_json: Path to posts data file\n",
        "        images_dir: Directory containing images\n",
        "        query_text: Text query for search (optional)\n",
        "        query_image: Image path for search (optional)\n",
        "        generate_image: Image path for content generation (optional)\n",
        "        generate_caption: Caption for content generation (optional)\n",
        "    \"\"\"\n",
        "    print(\"🚀 Starting Complete Multimodal RAG Workflow\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Step 1: Data Ingestion\n",
        "    print(\"\\n📥 Step 1: Data Ingestion\")\n",
        "    ingest(posts_json, images_dir)\n",
        "    \n",
        "    hits = []\n",
        "    \n",
        "    # Step 2: Search (Text or Image)\n",
        "    if query_text:\n",
        "        print(f\"\\n🔍 Step 2: Text Search for '{query_text}'\")\n",
        "        # Initial text search\n",
        "        hits = search_by_text(query_text, TOPK_INIT)\n",
        "        print(f\"Found {len(hits)} initial results\")\n",
        "        \n",
        "        # Advanced post-processing\n",
        "        hits = post_process_for_text_query(query_text, hits, images_dir)\n",
        "        print(f\"After reranking and diversification: {len(hits)} results\")\n",
        "        \n",
        "    elif query_image:\n",
        "        print(f\"\\n🔍 Step 2: Image Search for '{query_image}'\")\n",
        "        hits = search_by_image(query_image, TOPK_INIT)\n",
        "        print(f\"Found {len(hits)} similar images\")\n",
        "    \n",
        "    # Step 3: Display Results\n",
        "    if hits:\n",
        "        print(f\"\\n📋 Step 3: Search Results\")\n",
        "        print(\"-\" * 40)\n",
        "        for i, h in enumerate(hits, 1):\n",
        "            payload = h['payload']\n",
        "            print(f\"[{i}] Score: {h['score']:.3f}\")\n",
        "            print(f\"    Image: {payload.get('image', 'N/A')}\")\n",
        "            print(f\"    Caption: {payload.get('caption', 'N/A')[:80]}...\")\n",
        "            print(f\"    Description: {payload.get('description', 'N/A')[:80]}...\")\n",
        "            print()\n",
        "    \n",
        "    # Step 4: Content Generation\n",
        "    if generate_image and hits:\n",
        "        print(f\"\\n✨ Step 4: Content Generation\")\n",
        "        print(f\"Generating content for image: {generate_image}\")\n",
        "        print(f\"Caption: {generate_caption}\")\n",
        "        print(f\"Using {len(hits[:3])} examples as style reference...\")\n",
        "        \n",
        "        try:\n",
        "            generated_desc = gen_with_qwen_vl(generate_image, generate_caption, hits[:3])\n",
        "            print(f\"\\n🎯 Generated Description:\")\n",
        "            print(f\"'{generated_desc}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Generation failed: {e}\")\n",
        "    \n",
        "    print(\"\\n✅ Workflow Complete!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "print(\"Complete workflow function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Example Usage {#examples}\n",
        "\n",
        "Now let's run some examples to demonstrate the multimodal RAG system in action.\n",
        "\n",
        "**Note**: Make sure you have the data files (`posts_data.json` and images directory) in the correct location before running these examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting Complete Multimodal RAG Workflow\n",
            "============================================================\n",
            "\n",
            "📥 Step 1: Data Ingestion\n",
            "=== Starting Data Ingestion ===\n",
            "luffy ['/Users/sayems_mac/BrandReview/brandReviewModels/RAG_pipeline/images/2025-09-23.jpg', '/Users/sayems_mac/BrandReview/brandReviewModels/RAG_pipeline/images/2025-09-22.jpg']\n",
            "Loading posts from ../posts_data.json...\n",
            "Loaded 120 posts\n",
            "Preparing image paths and text content...\n",
            "Generating embeddings for 2 images...\n",
            "Loading CLIP ViT-L/14 model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image encoder loaded successfully!\n",
            "Generating embeddings for 120 text descriptions...\n",
            "Loading Multilingual E5 model...\n",
            "Text encoder loaded successfully!\n",
            "Image embeddings shape: (2, 768)\n",
            "Text embeddings shape: (120, 768)\n",
            "Using in-memory Qdrant database\n",
            "Creating collection 'mm_posts' with image dim=768, text dim=768\n",
            "Collection 'mm_posts' created successfully!\n",
            "Creating vector database points...\n",
            "Upserted 2 points to collection 'mm_posts'\n",
            "=== Ingestion Complete: 2 items stored ===\n",
            "\n",
            "🔍 Step 2: Text Search for 'rainy day scene'\n",
            "Found 2 initial results\n",
            "Post-processing 2 hits with cross-modal reranking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/mb/qsr33_nx15bbzcr3kfv4g67h0000gn/T/ipykernel_9034/1523765075.py:46: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  qc.recreate_collection(\n",
            "/var/folders/mb/qsr33_nx15bbzcr3kfv4g67h0000gn/T/ipykernel_9034/1223577332.py:13: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  res = qdrant().search(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected 2 diverse, non-duplicate results\n",
            "After reranking and diversification: 2 results\n",
            "\n",
            "📋 Step 3: Search Results\n",
            "----------------------------------------\n",
            "[1] Score: 0.845\n",
            "    Image: 2025-09-23.jpg\n",
            "    Caption: Us on a rainy day...\n",
            "    Description: Candid photo of an older man crouching beside a storm‑drain feeding puffed rice ...\n",
            "\n",
            "[2] Score: 0.756\n",
            "    Image: 2025-09-22.jpg\n",
            "    Caption: You got the order right Sir!...\n",
            "    Description: Black‑and‑white portrait of a man (Rahat) with a speech bubble: ‘Ordered boneles...\n",
            "\n",
            "\n",
            "✨ Step 4: Content Generation\n",
            "Generating content for image: ../images/2025-09-23.jpg\n",
            "Caption: oi mama !\n",
            "Using 2 examples as style reference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
            "/Users/sayems_mac/BrandReview/brandReviewModels/.venv/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2242: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]\n",
            "Some parameters are on the meta device because they were offloaded to the disk.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Generated Description:\n",
            "'system\n",
            "You are a social content writer. Write ONE vivid sentence (15–30 words), brand-safe, lightly witty, grounded in the new image/caption and inspired by prior examples. No hashtags/emojis; do not invent facts beyond what's visible or stated.\n",
            "user\n",
            "EXAMPLES:\n",
            "[1] CAPTION: Us on a rainy day\n",
            "    DESCRIPTION: Candid photo of an older man crouching beside a storm‑drain feeding puffed rice to a rat emerging from the grate; Delivery Hobe! logo is in the corner.\n",
            "[2] CAPTION: You got the order right Sir!\n",
            "    DESCRIPTION: Black‑and‑white portrait of a man (Rahat) with a speech bubble: ‘Ordered boneless chicken, they delivered Eggcellent service.’  The words ‘chicken’ and ‘egg’ are highlighted in red and a row of golden\n",
            "\n",
            "NEW CAPTION: oi mama !\n",
            "Write the description for this image:\n",
            "assistant\n",
            "A man feeds a rat a piece of bread while crouched next to a storm drain, with a colorful mural and a phone number in the corner.'\n",
            "\n",
            "✅ Workflow Complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Complete workflow with text search and content generation\n",
        "# Uncomment and modify the paths below to run this example\n",
        "\n",
        "\n",
        "# Set your data paths\n",
        "POSTS_FILE = \"../posts_data.json\"  # Path to your posts data\n",
        "IMAGES_DIR = \"/Users/sayems_mac/BrandReview/brandReviewModels/RAG_pipeline/images\"           # Path to your images directory\n",
        "\n",
        "# Example text query\n",
        "TEXT_QUERY = \"rainy day scene\"\n",
        "\n",
        "# Example content generation\n",
        "GENERATE_IMAGE = \"../images/2025-09-23.jpg\"  # Path to image for generation\n",
        "GENERATE_CAPTION = \"oi mama !\"\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def normalize_image_inputs(images_dir_or_list, recursive=True):\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\"}\n",
        "    def collect_dir(d: Path):\n",
        "        it = d.rglob(\"*\") if recursive else d.glob(\"*\")\n",
        "        return [str(fp) for fp in it if fp.is_file() and fp.suffix.lower() in exts]\n",
        "\n",
        "    if isinstance(images_dir_or_list, (list, tuple)):\n",
        "        files = []\n",
        "        for item in images_dir_or_list:\n",
        "            p = Path(item)\n",
        "            if p.is_dir():\n",
        "                files.extend(collect_dir(p))\n",
        "            elif p.is_file() and p.suffix.lower() in exts:\n",
        "                files.append(str(p))\n",
        "        return files\n",
        "\n",
        "    p = Path(images_dir_or_list)\n",
        "    if p.is_dir():\n",
        "        return collect_dir(p)\n",
        "    if p.is_file():\n",
        "        return [str(p)]\n",
        "    raise FileNotFoundError(f\"Path not found: {p}\")\n",
        "\n",
        "# Update your call:\n",
        "image_files = normalize_image_inputs(IMAGES_DIR) \n",
        "\n",
        "# Run the complete workflow\n",
        "\n",
        "run_complete_workflow(\n",
        "    posts_json=POSTS_FILE,\n",
        "    images_dir=image_files,\n",
        "    query_text=TEXT_QUERY,\n",
        "    generate_image=GENERATE_IMAGE,\n",
        "    generate_caption=GENERATE_CAPTION\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Individual function test code ready!\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Individual function usage\n",
        "# Uncomment sections below to test individual components\n",
        "\n",
        "\"\"\"\n",
        "# Test data loading\n",
        "print(\"=== Testing Data Loading ===\")\n",
        "posts = load_json_any(\"../posts_data.json\")\n",
        "print(f\"Loaded {len(posts)} posts\")\n",
        "if posts:\n",
        "    print(f\"Sample post keys: {list(posts[0].keys())}\")\n",
        "    print(f\"Sample post: {posts[0]}\")\n",
        "\n",
        "# Test embedding functions\n",
        "print(\"\\n=== Testing Embeddings ===\")\n",
        "if posts:\n",
        "    # Test text embedding\n",
        "    sample_text = posts[0].get('caption', '') + ' ' + posts[0].get('description', '')\n",
        "    text_emb = emb_text([sample_text], mode=\"passage\")\n",
        "    print(f\"Text embedding shape: {text_emb.shape}\")\n",
        "    \n",
        "    # Test image embedding (if image exists)\n",
        "    sample_image = f\"../images/{posts[0]['image']}\"\n",
        "    if os.path.exists(sample_image):\n",
        "        img_emb = emb_image([sample_image])\n",
        "        print(f\"Image embedding shape: {img_emb.shape}\")\n",
        "\n",
        "# Test search functions (after ingestion)\n",
        "print(\"\\n=== Testing Search ===\")\n",
        "# First run ingestion\n",
        "ingest(\"../posts_data.json\", \"../images\")\n",
        "\n",
        "# Test text search\n",
        "results = search_by_text(\"rainy day\", 5)\n",
        "print(f\"Text search returned {len(results)} results\")\n",
        "\n",
        "# Test image search (if image exists)\n",
        "if posts and os.path.exists(f\"../images/{posts[0]['image']}\"):\n",
        "    img_results = search_by_image(f\"../images/{posts[0]['image']}\", 5)\n",
        "    print(f\"Image search returned {len(img_results)} results\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"Individual function test code ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Advanced Features {#advanced}\n",
        "\n",
        "This section covers advanced features and customization options.\n",
        "\n",
        "### 11.1 Configuration Tuning\n",
        "\n",
        "**Retrieval Parameters:**\n",
        "- `TOPK_INIT`: Number of initial candidates (higher = more recall, slower)\n",
        "- `TOPK_FINAL`: Final number of results (lower = more focused)\n",
        "- `MMR_LAMBDA`: Diversification vs relevance balance (0.7 = good default)\n",
        "- `DEDUP_THR`: Deduplication threshold (0.96 = strict, 0.9 = lenient)\n",
        "\n",
        "**Model Selection:**\n",
        "- Image embeddings: CLIP variants (ViT-B/32, ViT-L/14, ViT-H/14)\n",
        "- Text embeddings: E5 variants (small, base, large) or other multilingual models\n",
        "- Generation: Qwen2.5-VL variants (3B, 7B, 14B, 72B)\n",
        "\n",
        "### 11.2 Performance Optimization\n",
        "\n",
        "**For Large Datasets:**\n",
        "- Use Qdrant server mode for persistence\n",
        "- Batch embedding generation\n",
        "- Implement embedding caching\n",
        "- Use GPU acceleration for models\n",
        "\n",
        "**For Production:**\n",
        "- Implement async processing\n",
        "- Add error handling and retries\n",
        "- Monitor embedding quality\n",
        "- Cache generated content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Advanced configuration examples ready!\n"
          ]
        }
      ],
      "source": [
        "# Advanced configuration examples\n",
        "# Uncomment and modify as needed\n",
        "\n",
        "\"\"\"\n",
        "# Performance tuning examples\n",
        "PERFORMANCE_CONFIG = {\n",
        "    # For faster processing (lower quality)\n",
        "    \"fast\": {\n",
        "        \"TOPK_INIT\": 12,\n",
        "        \"TOPK_FINAL\": 3,\n",
        "        \"MMR_LAMBDA\": 0.5,\n",
        "        \"DEDUP_THR\": 0.9,\n",
        "        \"IMG_EMB\": \"sentence-transformers/clip-ViT-B-32\",\n",
        "        \"GEN_MODEL_ID\": \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "    },\n",
        "    \n",
        "    # For high quality (slower processing)\n",
        "    \"quality\": {\n",
        "        \"TOPK_INIT\": 50,\n",
        "        \"TOPK_FINAL\": 10,\n",
        "        \"MMR_LAMBDA\": 0.8,\n",
        "        \"DEDUP_THR\": 0.98,\n",
        "        \"IMG_EMB\": \"sentence-transformers/clip-ViT-H-14\",\n",
        "        \"GEN_MODEL_ID\": \"Qwen/Qwen2.5-VL-72B-Instruct\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Apply configuration\n",
        "def apply_config(config_name):\n",
        "    config = PERFORMANCE_CONFIG[config_name]\n",
        "    globals().update(config)\n",
        "    print(f\"Applied {config_name} configuration\")\n",
        "\n",
        "# Example usage:\n",
        "# apply_config(\"fast\")  # For quick testing\n",
        "# apply_config(\"quality\")  # For production quality\n",
        "\"\"\"\n",
        "\n",
        "print(\"Advanced configuration examples ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Troubleshooting and Tips {#troubleshooting}\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "**Memory Issues:**\n",
        "- Use smaller models for development (CLIP ViT-B/32, Qwen2.5-VL-3B)\n",
        "- Process data in batches\n",
        "- Use `torch.no_grad()` for inference\n",
        "\n",
        "**Slow Performance:**\n",
        "- Enable GPU acceleration\n",
        "- Use in-memory Qdrant for small datasets\n",
        "- Cache embeddings to avoid recomputation\n",
        "- Reduce `TOPK_INIT` and `TOPK_FINAL`\n",
        "\n",
        "**Poor Search Results:**\n",
        "- Check embedding quality with sample queries\n",
        "- Adjust MMR_LAMBDA for better diversity/relevance balance\n",
        "- Verify text preprocessing (E5 prefixing)\n",
        "- Ensure image paths are correct\n",
        "\n",
        "**Generation Issues:**\n",
        "- Verify model compatibility with your hardware\n",
        "- Check image format (should be RGB)\n",
        "- Ensure sufficient GPU memory for generation model\n",
        "- Try different temperature and top_p values\n",
        "\n",
        "### Data Format Requirements\n",
        "\n",
        "**Posts JSON Format:**\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"image\": \"filename.jpg\",\n",
        "    \"caption\": \"Image caption\",\n",
        "    \"description\": \"Detailed description\",\n",
        "    \"date\": \"2025-01-01\",\n",
        "    \"engagement\": {\"likes\": 100, \"comments\": 5}\n",
        "  }\n",
        "]\n",
        "```\n",
        "\n",
        "**Image Requirements:**\n",
        "- Supported formats: JPG, PNG, WEBP\n",
        "- Automatic RGB conversion\n",
        "- Reasonable file sizes (< 10MB recommended)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Conclusion {#conclusion}\n",
        "\n",
        "This notebook demonstrates a complete multimodal RAG system that combines:\n",
        "\n",
        "### Key Features Implemented:\n",
        "- ✅ **Multimodal Embeddings**: CLIP for images, E5 for text\n",
        "- ✅ **Vector Database**: Qdrant with multi-vector support\n",
        "- ✅ **Advanced Retrieval**: MMR diversification and deduplication\n",
        "- ✅ **Cross-modal Reranking**: CLIP text-image similarity\n",
        "- ✅ **Content Generation**: Qwen2.5-VL for style-consistent output\n",
        "- ✅ **Multilingual Support**: E5 model supports multiple languages\n",
        "\n",
        "### System Architecture:\n",
        "```\n",
        "Data → Embeddings → Vector DB → Search → Rerank → Generate\n",
        "```\n",
        "\n",
        "### Use Cases:\n",
        "- **Content Creation**: Generate social media descriptions\n",
        "- **Image Search**: Find similar images by text or image\n",
        "- **Style Transfer**: Adapt content to match existing examples\n",
        "- **Multilingual RAG**: Support for non-English queries\n",
        "\n",
        "### Next Steps:\n",
        "1. **Scale Up**: Use server-mode Qdrant for production\n",
        "2. **Fine-tune**: Adapt models to your specific domain\n",
        "3. **Monitor**: Add logging and performance metrics\n",
        "4. **Deploy**: Create API endpoints for production use\n",
        "\n",
        "### Resources:\n",
        "- [Qdrant Documentation](https://qdrant.tech/documentation/)\n",
        "- [Sentence Transformers](https://www.sbert.net/)\n",
        "- [Qwen2.5-VL Model](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)\n",
        "- [CLIP Model](https://huggingface.co/sentence-transformers/clip-ViT-L-14)\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Building! 🚀**\n",
        "\n",
        "*This notebook provides a solid foundation for building sophisticated multimodal RAG applications. Feel free to modify and extend it for your specific needs.*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
